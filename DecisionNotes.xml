<?xml version="1.0" encoding="UTF-8"?>
<problem type="performance">
	<subject>Predecessor Layers as Inputs</subject>
	<body>
		There are two options
		1- Concatenate all the predecessor layers
		outputs as inputs for a
		specific layer then normally do the learning,
		weight algorithms
		2- not to concatenate and even ignore the Input
		variable and directly use the outputs for the matrices, but this will
		result in a more mess, it will mean that for each algorithm which uses
		the inputs we'll have to do specific algorithm implementation and
		that's very much wrong

		So the decision was to concatenate inputs and
		sacrifice the "near performance enhancement" for the "future
		performance enhancement"
	</body>
</problem>